<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Computer Vision for Trash Detection</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <div class="title">
            <h1>Analysis of Machine Learning Methods for Trash Detection</h1>
            <h2>Jerome Newhouse, Ryan Roche, Isaac Berlin, Robert Wang</h2>
        </div>
    </header>    
    <main>
        <section>
            <h2>Introduction</h2>
            <p>This project investigates the application of computer vision and machine learning methodologies 
            for automated waste detection and segregation. Four models—GroundingDINO, DETR, YOLOv8, and ResNet—are 
            trained and evaluated using the TACO dataset to identify the most effective approach for robust trash
            classification in real-world environments. The paper that accompanies this project is
            available <a href="/docs/computer_vision_project.pdf">here</a>.</p>
        </section>
        <section>
            <h2>Dataset</h2>
            <p>The <a href="http://tacodataset.org/">TACO</a> dataset (Trash Annotations in Context) is a 
            comprehensive collection of images depicting trash and recycling objects in real-world settings. 
            This dataset, used for training and evaluating our models, includes images containing between 0 and 40 
            objects spanning 19 classes, such as bottles, cans, and plastic bags. Additionally, it features a "catch-all" 
            class, labeled as unlabeled litter, for unidentified items. The dataset comprises 4,000 images, which were augmented 
            to expand the total to 6,000 images, all resized to 416x416 pixels. The annotations were created
            using Roboflow and are available <a href="https://universe.roboflow.com/divya-lzcld/taco-mqclx">here</a>.</p>
        </section>
        <section>
            <h2>Models Used</h2>
            <ul>
                <li><strong>GroundingDINO</strong> is a state-of-the-art transformer-based object detection model 
                designed for open-set detection tasks, integrating grounding capabilities to align visual features with textual
                prompts for precise and flexible object identification.</li>
                <br>
                <li><strong>DETR</strong> (DEtection TRansformer) is a transformer-based object detection model that eliminates
                the need for traditional hand-crafted components like anchor generation and NMS, using an end-to-end approach with a 
                bipartite matching loss to predict objects directly from input images.</li>
                <br>
                <li><strong>YOLO</strong> (You Only Look Once) is a real-time object detection framework that predicts 
                bounding boxes and class probabilities directly from full images in a single forward pass, offering high-speed 
                and accurate detection performance.</li>
                <br>
                <li><strong>ResNet</strong> (Residual Network) is a deep convolutional neural network architecture designed
                to address the vanishing gradient problem by introducing residual connections, enabling the training of very deep 
                networks while maintaining high performance in image classification and feature extraction tasks.</li>
            </ul>
        </section>
        <section>
            <h2>Key Findings</h2>
            <p>The YOLOv8 model achieved the highest precision and recall for real-time applications, outperforming other model types. 
            Smaller objects, such as bottle caps, remain challenging due to resolution limitations in the dataset and difficulty
            of the task. Our final results are available below.</p>
            <br>
            <table>
                <thead>
                    <tr>
                        <th>Evaluation Metric</th>
                        <th>YOLO</th>
                        <th>DETR</th>
                        <th>ResNet</th>
                        <th>GroundingDino</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Precision</td>
                        <td><strong>0.777</strong></td>
                        <td>0.612</td>
                        <td>0.503</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>Recall</td>
                        <td><strong>0.398</strong></td>
                        <td>0.285</td>
                        <td>0.379</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>mAP50</td>
                        <td><strong>0.491</strong></td>
                        <td>0.337</td>
                        <td>0.352</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>mAP50-95</td>
                        <td><strong>0.403</strong></td>
                        <td>0.260</td>
                        <td>0.297</td>
                        <td>N/A</td>
                    </tr>
                    <tr>
                        <td>FPS</td>
                        <td>200</td>
                        <td>13</td>
                        <td>10</td>
                        <td>2</td>
                    </tr>
                </tbody>
                <caption>Table 1: Evaluation metric scores for each model.</caption>
            </table>
        </section>
    </main>
    <footer>
    </footer>
</body>
</html>
