{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import supervision as sv\n",
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessor, AutoModelForObjectDetection, AutoImageProcessor\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from dataclasses import replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "processor = AutoImageProcessor.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = r\"C:\\Users\\isaac\\dev\\CV_Garbage_Detection\\Data\"\n",
    "\n",
    "ds_train = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=os.path.join(ds_path, \"train\"),\n",
    "    annotations_path=os.path.join(ds_path, \"train\", \"_annotations.coco.json\"),\n",
    ")\n",
    "\n",
    "ds_test = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=os.path.join(ds_path, \"test\"),\n",
    "    annotations_path=os.path.join(ds_path, \"test\", \"_annotations.coco.json\"),\n",
    ")\n",
    "\n",
    "ds_valid = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=os.path.join(ds_path, \"valid\"),\n",
    "    annotations_path=os.path.join(ds_path, \"valid\", \"_annotations.coco.json\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_train = A.Compose(\n",
    "    [\n",
    "        A.Perspective(p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        label_fields=[\"category\"],\n",
    "        clip=True,\n",
    "        min_area=25\n",
    "    ),\n",
    ")\n",
    "\n",
    "augmentation_valid = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        label_fields=[\"category\"],\n",
    "        clip=True,\n",
    "        min_area=1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_COUNT = 5\n",
    "\n",
    "for i in range(IMAGE_COUNT):\n",
    "    _, image, annotations = ds_train[i]\n",
    "\n",
    "    output = augmentation_train(\n",
    "        image=image,\n",
    "        bboxes=annotations.xyxy,\n",
    "        category=annotations.class_id\n",
    "    )\n",
    "\n",
    "    augmented_image = output[\"image\"]\n",
    "    augmented_annotations = replace(\n",
    "        annotations,\n",
    "        xyxy=np.array(output[\"bboxes\"]),\n",
    "        class_id=np.array(output[\"category\"])\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset, processor, transform):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations_as_coco(image_id, categories, boxes):\n",
    "        ...\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, image, annotations = self.dataset[idx]\n",
    "\n",
    "        image = image[:, :, ::-1]\n",
    "        transformed = self.transform(\n",
    "            image=image,\n",
    "            bboxes=annotations.xyxy,\n",
    "            category=annotations.class_id\n",
    "        )\n",
    "        image = transformed[\"image\"]\n",
    "        boxes = transformed[\"bboxes\"]\n",
    "        categories = transformed[\"category\"]\n",
    "\n",
    "        formatted_annotations = self.annotations_as_coco(\n",
    "            image_id=idx, \n",
    "            categories=categories, \n",
    "            boxes=boxes\n",
    "        )\n",
    "        result = self.processor(\n",
    "            images=image, \n",
    "            annotations=formatted_annotations, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {k: v[0] for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset_train = AugmentedDetectionDataset(\n",
    "    ds_train, processor, transform=augmentation_train)\n",
    "augmented_dataset_valid = AugmentedDetectionDataset(\n",
    "    ds_valid, processor, transform=augmentation_valid)\n",
    "augmented_dataset_test = AugmentedDetectionDataset(\n",
    "    ds_test, processor, transform=augmentation_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([\n",
    "        x[\"pixel_values\"] \n",
    "        for x \n",
    "        in batch]\n",
    "    )\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_r50vd_coco_o365 and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([20, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([19, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "id2label = {id: label for id, label in enumerate(ds_train.classes)}\n",
    "label2id = {label: id for id, label in enumerate(ds_train.classes)}\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSCI5561",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
